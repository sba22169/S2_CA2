{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9116907",
   "metadata": {},
   "source": [
    "# Twitter did not come back on my Academic research application.\n",
    "# So I proceeded to work with the Archive analysis from\n",
    "# https://archive.org/download/archiveteam-twitter-stream-2022-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f897097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import json\n",
    "import warnings\n",
    "import gzip\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faa82ca9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Volumes/TOSHIBA\\\\ EXT/archive'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Volumes/TOSHIBA\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m EXT/archive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m search_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrans\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 27\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mextract_json_from_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43msearch_word\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m, in \u001b[0;36mextract_json_from_directory\u001b[0;34m(directory, search_word, col)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Function to unzip and extract all json all files in a given directory\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m extracted_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json.gz\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      7\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, file_name)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Volumes/TOSHIBA\\\\ EXT/archive'"
     ]
    }
   ],
   "source": [
    "# Function\n",
    "def extract_json_from_directory(directory, search_word , col ):\n",
    "    \"\"\"Function to unzip and extract all json all files in a given directory\"\"\"\n",
    "    extracted_data = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.json.gz'):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            with gzip.open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    try:\n",
    "                        json_data = json.loads(line)\n",
    "                        extracted_data.append(json_data)\n",
    "\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error parsing JSON line in {file_name}: {e}\")\n",
    "        df = pd.DataFrame(extracted_data)\n",
    "        filtered = filter_tweets(df,search_word,col)\n",
    "    ftdf.append(filtered)\n",
    "        \n",
    "\n",
    "    \n",
    "        #for data in extracted_data:\n",
    "            #json.dump(data, output)\n",
    "    return ftdf\n",
    "directory = \"/Volumes/TOSHIBA\\ EXT/archive\"\n",
    "search_word = 'trans'\n",
    "df = extract_json_from_directory(directory,search_word,'text')\n",
    "\n",
    "# output_file = \"/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa1ace0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e519c316",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gtdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m cbdf \u001b[38;5;241m=\u001b[39m extract_json_from_directory(directory,search_words)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# gtdf = cbdf[['created_at','text']]\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[43mgtdf\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     38\u001b[0m gtdf\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSeaerched_tweets_result.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gtdf' is not defined"
     ]
    }
   ],
   "source": [
    "# Function copy to add jsonm search experiment ******************************************works\n",
    "def extract_json_from_directory(directory, search_words):\n",
    "    \"\"\"Function to unzip and extract all json all files in a given directory\"\"\"\n",
    "    extracted_data = []\n",
    "    my_df = pd.DataFrame()\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.json.gz'):\n",
    "            print(f\"Extracting file {file_name} \")\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            with gzip.open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    try:\n",
    "                        json_data = json.loads(line)\n",
    "                        extracted_data.append(json_data)\n",
    "\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error parsing JSON line in {file_name}: {e}\")\n",
    "                        \n",
    "                df = pd.DataFrame(extracted_data)\n",
    "                filtered_df = df[df[\"text\"].str.contains(\"gay|trans\")]\n",
    "                # filtered_df = df[df['text'].isin(search_words)]\n",
    "                \n",
    "                filtered_df = filtered_df[['created_at','text']]\n",
    "                my_df = pd.concat([filtered_df,my_df], ignore_index = True)\n",
    "                # my_df = my_df.append(filtered_df, ignore_index=True)\n",
    "             \n",
    "            \n",
    "            \n",
    "\n",
    "    return my_df\n",
    "\n",
    "\n",
    "search_words = ['trans','gay']\n",
    "directory = \"/Volumes/TOSHIBA_EXT/archive\"\n",
    "cbdf = extract_json_from_directory(directory,search_words)\n",
    "# gtdf = cbdf[['created_at','text']]\n",
    "gtdf.shape\n",
    "gtdf.to_csv('Seaerched_tweets_result.csv')\n",
    "\n",
    "# output_file = \"/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cecf4161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>id_str</th>\n",
       "      <th>text</th>\n",
       "      <th>display_text_range</th>\n",
       "      <th>source</th>\n",
       "      <th>truncated</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>...</th>\n",
       "      <th>lang</th>\n",
       "      <th>timestamp_ms</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>quoted_status</th>\n",
       "      <th>quoted_status_permalink</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>retweeted_status</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>extended_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wed Aug 31 23:59:55 +0000 2022</td>\n",
       "      <td>1565127274931933187</td>\n",
       "      <td>1565127274931933187</td>\n",
       "      <td>RT @mirelct: El @mppsp_vzla ha implementado el...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>es</td>\n",
       "      <td>1661990395659</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'created_at': 'Wed Aug 31 21:41:38 +0000 2022...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wed Aug 31 23:59:59 +0000 2022</td>\n",
       "      <td>1565127291738488833</td>\n",
       "      <td>1565127291738488833</td>\n",
       "      <td>@republikaonline Hukum mati, prosesnya secara ...</td>\n",
       "      <td>[17, 74]</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>1.564878e+18</td>\n",
       "      <td>1564878407057387521</td>\n",
       "      <td>22126902.0</td>\n",
       "      <td>...</td>\n",
       "      <td>in</td>\n",
       "      <td>1661990399666</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thu Sep 01 00:00:00 +0000 2022</td>\n",
       "      <td>1565127295911723008</td>\n",
       "      <td>1565127295911723008</td>\n",
       "      <td>Sem conseguir ler NADA e o período mal começou...</td>\n",
       "      <td>[0, 140]</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>pt</td>\n",
       "      <td>1661990400661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'full_text': 'Sem conseguir ler NADA e o perí...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thu Sep 01 00:00:04 +0000 2022</td>\n",
       "      <td>1565127312680665094</td>\n",
       "      <td>1565127312680665094</td>\n",
       "      <td>RT @juanjullian_: a transfóbica vai monetizar ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>pt</td>\n",
       "      <td>1661990404659</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'created_at': 'Wed Aug 31 17:05:10 +0000 2022...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thu Sep 01 00:00:07 +0000 2022</td>\n",
       "      <td>1565127325288636417</td>\n",
       "      <td>1565127325288636417</td>\n",
       "      <td>This is a huge change. \\n\\n“Student-athletes w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>1661990407665</td>\n",
       "      <td>1.565080e+18</td>\n",
       "      <td>1565079777052073985</td>\n",
       "      <td>{'created_at': 'Wed Aug 31 20:51:11 +0000 2022...</td>\n",
       "      <td>{'url': 'https://t.co/mvNLzJsLZD', 'expanded':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'full_text': 'This is a huge change. \n",
       "\n",
       "“Stude...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at                   id               id_str  \\\n",
       "0  Wed Aug 31 23:59:55 +0000 2022  1565127274931933187  1565127274931933187   \n",
       "1  Wed Aug 31 23:59:59 +0000 2022  1565127291738488833  1565127291738488833   \n",
       "2  Thu Sep 01 00:00:00 +0000 2022  1565127295911723008  1565127295911723008   \n",
       "3  Thu Sep 01 00:00:04 +0000 2022  1565127312680665094  1565127312680665094   \n",
       "4  Thu Sep 01 00:00:07 +0000 2022  1565127325288636417  1565127325288636417   \n",
       "\n",
       "                                                text display_text_range  \\\n",
       "0  RT @mirelct: El @mppsp_vzla ha implementado el...                NaN   \n",
       "1  @republikaonline Hukum mati, prosesnya secara ...           [17, 74]   \n",
       "2  Sem conseguir ler NADA e o período mal começou...           [0, 140]   \n",
       "3  RT @juanjullian_: a transfóbica vai monetizar ...                NaN   \n",
       "4  This is a huge change. \\n\\n“Student-athletes w...                NaN   \n",
       "\n",
       "                                              source  truncated  \\\n",
       "0  <a href=\"https://mobile.twitter.com\" rel=\"nofo...      False   \n",
       "1  <a href=\"http://twitter.com/download/android\" ...      False   \n",
       "2  <a href=\"http://twitter.com/download/android\" ...       True   \n",
       "3  <a href=\"http://twitter.com/download/iphone\" r...      False   \n",
       "4  <a href=\"http://twitter.com/download/iphone\" r...       True   \n",
       "\n",
       "   in_reply_to_status_id in_reply_to_status_id_str  in_reply_to_user_id  ...  \\\n",
       "0                    NaN                      None                  NaN  ...   \n",
       "1           1.564878e+18       1564878407057387521           22126902.0  ...   \n",
       "2                    NaN                      None                  NaN  ...   \n",
       "3                    NaN                      None                  NaN  ...   \n",
       "4                    NaN                      None                  NaN  ...   \n",
       "\n",
       "  lang   timestamp_ms quoted_status_id quoted_status_id_str  \\\n",
       "0   es  1661990395659              NaN                  NaN   \n",
       "1   in  1661990399666              NaN                  NaN   \n",
       "2   pt  1661990400661              NaN                  NaN   \n",
       "3   pt  1661990404659              NaN                  NaN   \n",
       "4   en  1661990407665     1.565080e+18  1565079777052073985   \n",
       "\n",
       "                                       quoted_status  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  {'created_at': 'Wed Aug 31 20:51:11 +0000 2022...   \n",
       "\n",
       "                             quoted_status_permalink possibly_sensitive  \\\n",
       "0                                                NaN                NaN   \n",
       "1                                                NaN                NaN   \n",
       "2                                                NaN              False   \n",
       "3                                                NaN                NaN   \n",
       "4  {'url': 'https://t.co/mvNLzJsLZD', 'expanded':...                NaN   \n",
       "\n",
       "                                    retweeted_status  extended_entities  \\\n",
       "0  {'created_at': 'Wed Aug 31 21:41:38 +0000 2022...                NaN   \n",
       "1                                                NaN                NaN   \n",
       "2                                                NaN                NaN   \n",
       "3  {'created_at': 'Wed Aug 31 17:05:10 +0000 2022...                NaN   \n",
       "4                                                NaN                NaN   \n",
       "\n",
       "                                      extended_tweet  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2  {'full_text': 'Sem conseguir ler NADA e o perí...  \n",
       "3                                                NaN  \n",
       "4  {'full_text': 'This is a huge change. \n",
       "\n",
       "“Stude...  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4de237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test loading json and searching in one step\n",
    "import json\n",
    "\n",
    "def search_json(json_file, search_terms):\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    results = []\n",
    "    for item in data:\n",
    "        text = item.get('text', '')  # Change 'text' to the appropriate key in your JSON structure\n",
    "        if all(term in text.lower() for term in search_terms):\n",
    "            results.append(text)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "json_file = 'data.json'  # Replace with your JSON file path\n",
    "search_terms = ['term1', 'term2', 'term3']  # Replace with your search terms\n",
    "\n",
    "matching_texts = search_json(json_file, search_terms)\n",
    "if matching_texts:\n",
    "    print(\"Matching texts:\")\n",
    "    for text in matching_texts:\n",
    "        print(text)\n",
    "else:\n",
    "    print(\"No matching texts found.\")\n",
    "```\n",
    "\n",
    "Make sure to replace `'text'` with the appropriate key that contains the text string within your JSON structure. Also, update the `json_file` variable with the path to your actual JSON file, and `search_terms` with the set of terms you want to search for.\n",
    "\n",
    "The script will print all the text strings from the JSON file that contain all the search terms. If no matching texts are found, it will display \"No matching texts found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "755bb40a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Trailing data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_results\n\u001b[1;32m     11\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data/20220901\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m cbdf \u001b[38;5;241m=\u001b[39m \u001b[43mload_json_files_and_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m cbdf\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36mload_json_files_and_search\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      5\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, file_name)\n\u001b[0;32m----> 6\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     search_results \u001b[38;5;241m=\u001b[39m filter_tweets(df,search_word,col)\n\u001b[1;32m      8\u001b[0m     combined_results \u001b[38;5;241m=\u001b[39m combined_results\u001b[38;5;241m.\u001b[39mappend(search_results, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/json/_json.py:757\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m json_reader:\n\u001b[0;32m--> 757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/json/_json.py:915\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_lines(data_lines))\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 915\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/json/_json.py:937\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    935\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 937\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/json/_json.py:1064\u001b[0m, in \u001b[0;36mParser.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_numpy()\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_no_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/json/_json.py:1321\u001b[0m, in \u001b[0;36mFrameParser._parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1317\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[0;32m-> 1321\u001b[0m         \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m     )\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1324\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1325\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[1;32m   1326\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1327\u001b[0m     }\n",
      "\u001b[0;31mValueError\u001b[0m: Trailing data"
     ]
    }
   ],
   "source": [
    "def load_json_files_and_search(directory):\n",
    "    combined_results = pd.DataFrame()\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.json.gz'):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            df = pd.read_json(file_path)\n",
    "            search_results = filter_tweets(df,search_word,col)\n",
    "            combined_results = combined_results.append(search_results, ignore_index=True)\n",
    "    return combined_results\n",
    "\n",
    "directory = \"/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data/20220901\"\n",
    "cbdf = load_json_files_and_search(directory)\n",
    "cbdf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ed42292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function loads the json files one at a time and converts them into dataframe\n",
    "def convert_json_to_dataframe(json_file):\n",
    "    \"\"\"A simple function to convert json file to dataframe\"\"\"\n",
    "    data = []\n",
    "    with open(json_file, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                json_data = json.loads(line)\n",
    "                data.append(json_data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing JSON line: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n",
    "# twit_file = '20220901000000.json'\n",
    "# Keeping the json files outside my git repository , to reduce git size\n",
    "#twit_file = \"/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data/20220901/20220901000000.json\"\n",
    "#df = convert_json_to_dataframe(twit_file)\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac47b595",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "extract_json_from_directory() missing 2 required positional arguments: 'search_word' and 'col'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data/20220901\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Call function to extract tweet directory\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m combined_json \u001b[38;5;241m=\u001b[39m \u001b[43mextract_json_from_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: extract_json_from_directory() missing 2 required positional arguments: 'search_word' and 'col'"
     ]
    }
   ],
   "source": [
    "# path to my zipped tweets\n",
    "directory = \"/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data/20220901\"\n",
    "# Call function to extract tweet directory\n",
    "combined_json = extract_json_from_directory(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9916e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = convert_json_to_dataframe(combined_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa8bd60",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m search_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgas\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m fdf \u001b[38;5;241m=\u001b[39m filter_tweets(\u001b[43mdf\u001b[49m, col , search_word)\n\u001b[1;32m      9\u001b[0m fdf\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "def filter_tweets(df, col , search_word):\n",
    "    filtered_tweets = df[df[col].apply(lambda x: search_word.lower() in x.lower().split())]\n",
    "    return filtered_tweets\n",
    "\n",
    "\n",
    "search_word = 'gas'\n",
    "col = 'text'\n",
    "fdf = filter_tweets(df, col , search_word)\n",
    "fdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8392a47a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
