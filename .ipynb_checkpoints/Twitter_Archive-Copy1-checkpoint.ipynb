{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9116907",
   "metadata": {},
   "source": [
    "# Twitter did not come back on my Academic research application.\n",
    "# So I proceeded to work with the Archive analysis from\n",
    "# https://archive.org/download/archiveteam-twitter-stream-2022-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f897097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import json\n",
    "import warnings\n",
    "import gzip\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faa82ca9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filter_tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data/20220901\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m search_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrans\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 27\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mextract_json_from_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43msearch_word\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m, in \u001b[0;36mextract_json_from_directory\u001b[0;34m(directory, search_word, col)\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError parsing JSON line in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(extracted_data)\n\u001b[0;32m---> 17\u001b[0m     filtered \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_tweets\u001b[49m(df,search_word,col)\n\u001b[1;32m     18\u001b[0m ftdf\u001b[38;5;241m.\u001b[39mappend(filtered)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m#for data in extracted_data:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;66;03m#json.dump(data, output)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filter_tweets' is not defined"
     ]
    }
   ],
   "source": [
    "# Function\n",
    "def extract_json_from_directory(directory, search_word , col ):\n",
    "    \"\"\"Function to unzip and extract all json all files in a given directory\"\"\"\n",
    "    extracted_data = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.json.gz'):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            with gzip.open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    try:\n",
    "                        json_data = json.loads(line)\n",
    "                        extracted_data.append(json_data)\n",
    "\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error parsing JSON line in {file_name}: {e}\")\n",
    "        df = pd.DataFrame(extracted_data)\n",
    "        filtered = filter_tweets(df,search_word,col)\n",
    "    ftdf.append(filtered)\n",
    "        \n",
    "\n",
    "    \n",
    "        #for data in extracted_data:\n",
    "            #json.dump(data, output)\n",
    "    return ftdf\n",
    "directory = \"/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data/20220901\"\n",
    "search_word = 'trans'\n",
    "df = extract_json_from_directory(directory,search_word,'text')\n",
    "\n",
    "# output_file = \"/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa1ace0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e519c316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting file 20220901072000.json.gz \n",
      "Extracting file 20220901051600.json.gz \n",
      "Extracting file 20220901203900.json.gz \n",
      "Extracting file 20220901234900.json.gz \n",
      "Extracting file 20220901045000.json.gz \n",
      "Extracting file 20220901145600.json.gz \n",
      "Extracting file 20220901151000.json.gz \n",
      "Extracting file 20220901172600.json.gz \n",
      "Extracting file 20220901233300.json.gz \n",
      "Extracting file 20220901033600.json.gz \n",
      "Extracting file 20220901210500.json.gz \n",
      "Extracting file 20220901093100.json.gz \n",
      "Extracting file 20220901010000.json.gz \n",
      "Extracting file 20220901004600.json.gz \n",
      "Extracting file 20220901204300.json.gz \n",
      "Extracting file 20220901104000.json.gz \n",
      "Extracting file 20220901193700.json.gz \n",
      "Extracting file 20220901110600.json.gz \n",
      "Extracting file 20220901133000.json.gz \n",
      "Extracting file 20220901132400.json.gz \n",
      "Extracting file 20220901143800.json.gz \n",
      "Extracting file 20220901111200.json.gz \n",
      "Extracting file 20220901192300.json.gz \n",
      "Extracting file 20220901174800.json.gz \n",
      "Extracting file 20220901105400.json.gz \n",
      "Extracting file 20220901205700.json.gz \n",
      "Extracting file 20220901005200.json.gz \n",
      "Extracting file 20220901011400.json.gz \n",
      "Extracting file 20220901092500.json.gz \n",
      "Extracting file 20220901060800.json.gz \n",
      "Extracting file 20220901032200.json.gz \n",
      "Extracting file 20220901232700.json.gz \n",
      "Extracting file 20220901173200.json.gz \n",
      "Extracting file 20220901121800.json.gz \n",
      "Extracting file 20220901150400.json.gz \n",
      "Extracting file 20220901144200.json.gz \n",
      "Extracting file 20220901195900.json.gz \n",
      "Extracting file 20220901044400.json.gz \n",
      "Extracting file 20220901035800.json.gz \n",
      "Extracting file 20220901081900.json.gz \n",
      "Extracting file 20220901050200.json.gz \n",
      "Extracting file 20220901002800.json.gz \n",
      "Extracting file 20220901073400.json.gz \n",
      "Extracting file 20220901190200.json.gz \n",
      "Extracting file 20220901113300.json.gz \n",
      "Extracting file 20220901141900.json.gz \n",
      "Extracting file 20220901130500.json.gz \n",
      "Extracting file 20220901184400.json.gz \n",
      "Extracting file 20220901124300.json.gz \n",
      "Extracting file 20220901024500.json.gz \n",
      "Extracting file 20220901224000.json.gz \n",
      "Extracting file 20220901055900.json.gz \n",
      "Extracting file 20220901084200.json.gz \n",
      "Extracting file 20220901230600.json.gz \n",
      "Extracting file 20220901030300.json.gz \n",
      "Extracting file 20220901062900.json.gz \n",
      "Extracting file 20220901090400.json.gz \n",
      "Extracting file 20220901213000.json.gz \n",
      "Extracting file 20220901013500.json.gz \n",
      "Extracting file 20220901152500.json.gz \n",
      "Extracting file 20220901123900.json.gz \n",
      "Extracting file 20220901171300.json.gz \n",
      "Extracting file 20220901114900.json.gz \n",
      "Extracting file 20220901165500.json.gz \n",
      "Extracting file 20220901065300.json.gz \n",
      "Extracting file 20220901071500.json.gz \n",
      "Extracting file 20220901000900.json.gz \n",
      "Extracting file 20220901052300.json.gz \n",
      "Extracting file 20220901083800.json.gz \n",
      "Extracting file 20220901201800.json.gz \n",
      "Extracting file 20220901053700.json.gz \n",
      "Extracting file 20220901070100.json.gz \n",
      "Extracting file 20220901064700.json.gz \n",
      "Extracting file 20220901164100.json.gz \n",
      "Extracting file 20220901170700.json.gz \n",
      "Extracting file 20220901153100.json.gz \n",
      "Extracting file 20220901012100.json.gz \n",
      "Extracting file 20220901091000.json.gz \n",
      "Extracting file 20220901212400.json.gz \n",
      "Extracting file 20220901031700.json.gz \n",
      "Extracting file 20220901231200.json.gz \n",
      "Extracting file 20220901085600.json.gz \n",
      "Extracting file 20220901225400.json.gz \n",
      "Extracting file 20220901025100.json.gz \n",
      "Extracting file 20220901125700.json.gz \n",
      "Extracting file 20220901185000.json.gz \n",
      "Extracting file 20220901131100.json.gz \n",
      "Extracting file 20220901112700.json.gz \n",
      "Extracting file 20220901191600.json.gz \n",
      "Extracting file 20220901120100.json.gz \n",
      "Extracting file 20220901180600.json.gz \n",
      "Extracting file 20220901103700.json.gz \n",
      "Extracting file 20220901134700.json.gz \n",
      "Extracting file 20220901194000.json.gz \n",
      "Extracting file 20220901094600.json.gz \n",
      "Extracting file 20220901034100.json.gz \n",
      "Extracting file 20220901234400.json.gz \n",
      "Extracting file 20220901080000.json.gz \n",
      "Extracting file 20220901203400.json.gz \n",
      "Extracting file 20220901003100.json.gz \n",
      "Extracting file 20220901220200.json.gz \n",
      "Extracting file 20220901020700.json.gz \n",
      "Extracting file 20220901161700.json.gz \n",
      "Extracting file 20220901142100.json.gz \n",
      "Extracting file 20220901175100.json.gz \n",
      "Extracting file 20220901075700.json.gz \n",
      "Extracting file 20220901042700.json.gz \n",
      "Extracting file 20220901210800.json.gz \n",
      "Extracting file 20220901061100.json.gz \n",
      "Extracting file 20220901060500.json.gz \n",
      "Extracting file 20220901092800.json.gz \n",
      "Extracting file 20220901011900.json.gz \n",
      "Extracting file 20220901043300.json.gz \n",
      "Extracting file 20220901074300.json.gz \n",
      "Extracting file 20220901105900.json.gz \n",
      "Extracting file 20220901174500.json.gz \n",
      "Extracting file 20220901143500.json.gz \n",
      "Extracting file 20220901132900.json.gz \n",
      "Extracting file 20220901160300.json.gz \n",
      "Extracting file 20220901021300.json.gz \n",
      "Extracting file 20220901073900.json.gz \n",
      "Extracting file 20220901221600.json.gz \n",
      "Extracting file 20220901002500.json.gz \n",
      "Extracting file 20220901081400.json.gz \n",
      "Extracting file 20220901202000.json.gz \n",
      "Extracting file 20220901235000.json.gz \n",
      "Extracting file 20220901035500.json.gz \n",
      "Extracting file 20220901095200.json.gz \n",
      "Extracting file 20220901044900.json.gz \n",
      "Extracting file 20220901195400.json.gz \n",
      "Extracting file 20220901135300.json.gz \n",
      "Extracting file 20220901102300.json.gz \n",
      "Extracting file 20220901150900.json.gz \n",
      "Extracting file 20220901181200.json.gz \n",
      "Extracting file 20220901121500.json.gz \n",
      "Extracting file 20220901041200.json.gz \n",
      "Extracting file 20220901013800.json.gz \n",
      "Extracting file 20220901090900.json.gz \n",
      "Extracting file 20220901062400.json.gz \n",
      "Extracting file 20220901055400.json.gz \n",
      "Extracting file 20220901024800.json.gz \n",
      "Extracting file 20220901184900.json.gz \n",
      "Extracting file 20220901155200.json.gz \n",
      "Extracting file 20220901162200.json.gz \n",
      "Extracting file 20220901130800.json.gz \n",
      "Extracting file 20220901141400.json.gz \n",
      "Extracting file 20220901200100.json.gz \n",
      "Extracting file 20220901083500.json.gz \n",
      "Extracting file 20220901000400.json.gz \n",
      "Extracting file 20220901223700.json.gz \n",
      "Extracting file 20220901071800.json.gz \n",
      "Extracting file 20220901023200.json.gz \n",
      "Extracting file 20220901014200.json.gz \n",
      "Extracting file 20220901214700.json.gz \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m search_words \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrans\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgay\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     34\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data/20220901\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 35\u001b[0m cbdf \u001b[38;5;241m=\u001b[39m \u001b[43mextract_json_from_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43msearch_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# gtdf = cbdf[['created_at','text']]\u001b[39;00m\n\u001b[1;32m     37\u001b[0m gtdf\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m, in \u001b[0;36mextract_json_from_directory\u001b[0;34m(directory, search_words)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError parsing JSON line in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextracted_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgay|trans\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# filtered_df = df[df['text'].isin(search_words)]\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:746\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    745\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 746\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[1;32m    748\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[1;32m    749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    752\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    754\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    755\u001b[0m         arrays,\n\u001b[1;32m    756\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    759\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    760\u001b[0m     )\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py:510\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 510\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py:867\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    865\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data[\u001b[38;5;241m0\u001b[39m], abc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[0;32m--> 867\u001b[0m     arr, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_list_of_dict_to_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m    869\u001b[0m     arr, columns \u001b[38;5;241m=\u001b[39m _list_of_series_to_arrays(data, columns)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py:947\u001b[0m, in \u001b[0;36m_list_of_dict_to_arrays\u001b[0;34m(data, columns)\u001b[0m\n\u001b[1;32m    945\u001b[0m     gen \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlist\u001b[39m(x\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data)\n\u001b[1;32m    946\u001b[0m     sort \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(d, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data)\n\u001b[0;32m--> 947\u001b[0m     pre_cols \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfast_unique_multiple_list_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    948\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(pre_cols)\n\u001b[1;32m    950\u001b[0m \u001b[38;5;66;03m# assure that they are of the base dict class and not of derived\u001b[39;00m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;66;03m# classes\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/lib.pyx:419\u001b[0m, in \u001b[0;36mpandas._libs.lib.fast_unique_multiple_list_gen\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py:945\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;124;03mConvert list of dicts to numpy arrays\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;124;03mcolumns : Index\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m     gen \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data)\n\u001b[1;32m    946\u001b[0m     sort \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(d, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data)\n\u001b[1;32m    947\u001b[0m     pre_cols \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mfast_unique_multiple_list_gen(gen, sort\u001b[38;5;241m=\u001b[39msort)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function copy to add jsonm search experiment ******************************************works\n",
    "def extract_json_from_directory(directory, search_words):\n",
    "    \"\"\"Function to unzip and extract all json all files in a given directory\"\"\"\n",
    "    extracted_data = []\n",
    "    my_df = pd.DataFrame()\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.json.gz'):\n",
    "            print(f\"Extracting file {file_name} \")\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            with gzip.open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    try:\n",
    "                        json_data = json.loads(line)\n",
    "                        extracted_data.append(json_data)\n",
    "\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error parsing JSON line in {file_name}: {e}\")\n",
    "                        \n",
    "                df = pd.DataFrame(extracted_data)\n",
    "                filtered_df = df[df[\"text\"].str.contains(\"gay|trans\")]\n",
    "                # filtered_df = df[df['text'].isin(search_words)]\n",
    "                \n",
    "                filtered_df = filtered_df[['created_at','text']]\n",
    "                my_df = pd.concat([filtered_df,my_df], ignore_index = True)\n",
    "                # my_df = my_df.append(filtered_df, ignore_index=True)\n",
    "             \n",
    "            \n",
    "            \n",
    "\n",
    "    return my_df\n",
    "\n",
    "\n",
    "search_words = ['trans','gay']\n",
    "directory = \"/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data/20220901\"\n",
    "cbdf = extract_json_from_directory(directory,search_words)\n",
    "# gtdf = cbdf[['created_at','text']]\n",
    "gtdf.shape\n",
    "gtdf.to_csv('Seaerched_tweets_result.csv')\n",
    "\n",
    "# output_file = \"/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cecf4161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>id_str</th>\n",
       "      <th>text</th>\n",
       "      <th>display_text_range</th>\n",
       "      <th>source</th>\n",
       "      <th>truncated</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>...</th>\n",
       "      <th>lang</th>\n",
       "      <th>timestamp_ms</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>quoted_status</th>\n",
       "      <th>quoted_status_permalink</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>retweeted_status</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>extended_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wed Aug 31 23:59:55 +0000 2022</td>\n",
       "      <td>1565127274931933187</td>\n",
       "      <td>1565127274931933187</td>\n",
       "      <td>RT @mirelct: El @mppsp_vzla ha implementado el...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>es</td>\n",
       "      <td>1661990395659</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'created_at': 'Wed Aug 31 21:41:38 +0000 2022...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wed Aug 31 23:59:59 +0000 2022</td>\n",
       "      <td>1565127291738488833</td>\n",
       "      <td>1565127291738488833</td>\n",
       "      <td>@republikaonline Hukum mati, prosesnya secara ...</td>\n",
       "      <td>[17, 74]</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>1.564878e+18</td>\n",
       "      <td>1564878407057387521</td>\n",
       "      <td>22126902.0</td>\n",
       "      <td>...</td>\n",
       "      <td>in</td>\n",
       "      <td>1661990399666</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thu Sep 01 00:00:00 +0000 2022</td>\n",
       "      <td>1565127295911723008</td>\n",
       "      <td>1565127295911723008</td>\n",
       "      <td>Sem conseguir ler NADA e o período mal começou...</td>\n",
       "      <td>[0, 140]</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>pt</td>\n",
       "      <td>1661990400661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'full_text': 'Sem conseguir ler NADA e o perí...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thu Sep 01 00:00:04 +0000 2022</td>\n",
       "      <td>1565127312680665094</td>\n",
       "      <td>1565127312680665094</td>\n",
       "      <td>RT @juanjullian_: a transfóbica vai monetizar ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>pt</td>\n",
       "      <td>1661990404659</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'created_at': 'Wed Aug 31 17:05:10 +0000 2022...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thu Sep 01 00:00:07 +0000 2022</td>\n",
       "      <td>1565127325288636417</td>\n",
       "      <td>1565127325288636417</td>\n",
       "      <td>This is a huge change. \\n\\n“Student-athletes w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>1661990407665</td>\n",
       "      <td>1.565080e+18</td>\n",
       "      <td>1565079777052073985</td>\n",
       "      <td>{'created_at': 'Wed Aug 31 20:51:11 +0000 2022...</td>\n",
       "      <td>{'url': 'https://t.co/mvNLzJsLZD', 'expanded':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'full_text': 'This is a huge change. \n",
       "\n",
       "“Stude...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at                   id               id_str  \\\n",
       "0  Wed Aug 31 23:59:55 +0000 2022  1565127274931933187  1565127274931933187   \n",
       "1  Wed Aug 31 23:59:59 +0000 2022  1565127291738488833  1565127291738488833   \n",
       "2  Thu Sep 01 00:00:00 +0000 2022  1565127295911723008  1565127295911723008   \n",
       "3  Thu Sep 01 00:00:04 +0000 2022  1565127312680665094  1565127312680665094   \n",
       "4  Thu Sep 01 00:00:07 +0000 2022  1565127325288636417  1565127325288636417   \n",
       "\n",
       "                                                text display_text_range  \\\n",
       "0  RT @mirelct: El @mppsp_vzla ha implementado el...                NaN   \n",
       "1  @republikaonline Hukum mati, prosesnya secara ...           [17, 74]   \n",
       "2  Sem conseguir ler NADA e o período mal começou...           [0, 140]   \n",
       "3  RT @juanjullian_: a transfóbica vai monetizar ...                NaN   \n",
       "4  This is a huge change. \\n\\n“Student-athletes w...                NaN   \n",
       "\n",
       "                                              source  truncated  \\\n",
       "0  <a href=\"https://mobile.twitter.com\" rel=\"nofo...      False   \n",
       "1  <a href=\"http://twitter.com/download/android\" ...      False   \n",
       "2  <a href=\"http://twitter.com/download/android\" ...       True   \n",
       "3  <a href=\"http://twitter.com/download/iphone\" r...      False   \n",
       "4  <a href=\"http://twitter.com/download/iphone\" r...       True   \n",
       "\n",
       "   in_reply_to_status_id in_reply_to_status_id_str  in_reply_to_user_id  ...  \\\n",
       "0                    NaN                      None                  NaN  ...   \n",
       "1           1.564878e+18       1564878407057387521           22126902.0  ...   \n",
       "2                    NaN                      None                  NaN  ...   \n",
       "3                    NaN                      None                  NaN  ...   \n",
       "4                    NaN                      None                  NaN  ...   \n",
       "\n",
       "  lang   timestamp_ms quoted_status_id quoted_status_id_str  \\\n",
       "0   es  1661990395659              NaN                  NaN   \n",
       "1   in  1661990399666              NaN                  NaN   \n",
       "2   pt  1661990400661              NaN                  NaN   \n",
       "3   pt  1661990404659              NaN                  NaN   \n",
       "4   en  1661990407665     1.565080e+18  1565079777052073985   \n",
       "\n",
       "                                       quoted_status  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  {'created_at': 'Wed Aug 31 20:51:11 +0000 2022...   \n",
       "\n",
       "                             quoted_status_permalink possibly_sensitive  \\\n",
       "0                                                NaN                NaN   \n",
       "1                                                NaN                NaN   \n",
       "2                                                NaN              False   \n",
       "3                                                NaN                NaN   \n",
       "4  {'url': 'https://t.co/mvNLzJsLZD', 'expanded':...                NaN   \n",
       "\n",
       "                                    retweeted_status  extended_entities  \\\n",
       "0  {'created_at': 'Wed Aug 31 21:41:38 +0000 2022...                NaN   \n",
       "1                                                NaN                NaN   \n",
       "2                                                NaN                NaN   \n",
       "3  {'created_at': 'Wed Aug 31 17:05:10 +0000 2022...                NaN   \n",
       "4                                                NaN                NaN   \n",
       "\n",
       "                                      extended_tweet  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2  {'full_text': 'Sem conseguir ler NADA e o perí...  \n",
       "3                                                NaN  \n",
       "4  {'full_text': 'This is a huge change. \n",
       "\n",
       "“Stude...  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4de237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test loading json and searching in one step\n",
    "import json\n",
    "\n",
    "def search_json(json_file, search_terms):\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    results = []\n",
    "    for item in data:\n",
    "        text = item.get('text', '')  # Change 'text' to the appropriate key in your JSON structure\n",
    "        if all(term in text.lower() for term in search_terms):\n",
    "            results.append(text)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "json_file = 'data.json'  # Replace with your JSON file path\n",
    "search_terms = ['term1', 'term2', 'term3']  # Replace with your search terms\n",
    "\n",
    "matching_texts = search_json(json_file, search_terms)\n",
    "if matching_texts:\n",
    "    print(\"Matching texts:\")\n",
    "    for text in matching_texts:\n",
    "        print(text)\n",
    "else:\n",
    "    print(\"No matching texts found.\")\n",
    "```\n",
    "\n",
    "Make sure to replace `'text'` with the appropriate key that contains the text string within your JSON structure. Also, update the `json_file` variable with the path to your actual JSON file, and `search_terms` with the set of terms you want to search for.\n",
    "\n",
    "The script will print all the text strings from the JSON file that contain all the search terms. If no matching texts are found, it will display \"No matching texts found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "755bb40a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Trailing data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_results\n\u001b[1;32m     11\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data/20220901\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m cbdf \u001b[38;5;241m=\u001b[39m \u001b[43mload_json_files_and_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m cbdf\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36mload_json_files_and_search\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      5\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, file_name)\n\u001b[0;32m----> 6\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     search_results \u001b[38;5;241m=\u001b[39m filter_tweets(df,search_word,col)\n\u001b[1;32m      8\u001b[0m     combined_results \u001b[38;5;241m=\u001b[39m combined_results\u001b[38;5;241m.\u001b[39mappend(search_results, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/json/_json.py:757\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m json_reader:\n\u001b[0;32m--> 757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/json/_json.py:915\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_lines(data_lines))\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 915\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/json/_json.py:937\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    935\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 937\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/json/_json.py:1064\u001b[0m, in \u001b[0;36mParser.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_numpy()\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_no_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/json/_json.py:1321\u001b[0m, in \u001b[0;36mFrameParser._parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1317\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[0;32m-> 1321\u001b[0m         \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m     )\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1324\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1325\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[1;32m   1326\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1327\u001b[0m     }\n",
      "\u001b[0;31mValueError\u001b[0m: Trailing data"
     ]
    }
   ],
   "source": [
    "def load_json_files_and_search(directory):\n",
    "    combined_results = pd.DataFrame()\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.json.gz'):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            df = pd.read_json(file_path)\n",
    "            search_results = filter_tweets(df,search_word,col)\n",
    "            combined_results = combined_results.append(search_results, ignore_index=True)\n",
    "    return combined_results\n",
    "\n",
    "directory = \"/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data/20220901\"\n",
    "cbdf = load_json_files_and_search(directory)\n",
    "cbdf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ed42292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function loads the json files one at a time and converts them into dataframe\n",
    "def convert_json_to_dataframe(json_file):\n",
    "    \"\"\"A simple function to convert json file to dataframe\"\"\"\n",
    "    data = []\n",
    "    with open(json_file, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                json_data = json.loads(line)\n",
    "                data.append(json_data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing JSON line: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n",
    "# twit_file = '20220901000000.json'\n",
    "# Keeping the json files outside my git repository , to reduce git size\n",
    "#twit_file = \"/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data/20220901/20220901000000.json\"\n",
    "#df = convert_json_to_dataframe(twit_file)\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac47b595",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "extract_json_from_directory() missing 2 required positional arguments: 'search_word' and 'col'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data/20220901\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Call function to extract tweet directory\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m combined_json \u001b[38;5;241m=\u001b[39m \u001b[43mextract_json_from_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: extract_json_from_directory() missing 2 required positional arguments: 'search_word' and 'col'"
     ]
    }
   ],
   "source": [
    "# path to my zipped tweets\n",
    "directory = \"/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data/20220901\"\n",
    "# Call function to extract tweet directory\n",
    "combined_json = extract_json_from_directory(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9916e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = convert_json_to_dataframe(combined_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa8bd60",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m search_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgas\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m fdf \u001b[38;5;241m=\u001b[39m filter_tweets(\u001b[43mdf\u001b[49m, col , search_word)\n\u001b[1;32m      9\u001b[0m fdf\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "def filter_tweets(df, col , search_word):\n",
    "    filtered_tweets = df[df[col].apply(lambda x: search_word.lower() in x.lower().split())]\n",
    "    return filtered_tweets\n",
    "\n",
    "\n",
    "search_word = 'gas'\n",
    "col = 'text'\n",
    "fdf = filter_tweets(df, col , search_word)\n",
    "fdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8392a47a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
