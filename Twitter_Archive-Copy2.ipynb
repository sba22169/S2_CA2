{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9116907",
   "metadata": {},
   "source": [
    "# Twitter did not come back on my Academic research application.\n",
    "# So I proceeded to work with the Archive analysis from\n",
    "# https://archive.org/download/archiveteam-twitter-stream-2022-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34718791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sc master - running locally\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07d79142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2554fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "import pyspark\n",
    "\n",
    "# spark is from the previous example.\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f897097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import json\n",
    "import warnings\n",
    "import gzip\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faa82ca9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Volumes/TOSHIBA\\\\ EXT/archive'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Volumes/TOSHIBA\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m EXT/archive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m search_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrans\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 27\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mextract_json_from_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43msearch_word\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m, in \u001b[0;36mextract_json_from_directory\u001b[0;34m(directory, search_word, col)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Function to unzip and extract all json all files in a given directory\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m extracted_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json.gz\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      7\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, file_name)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Volumes/TOSHIBA\\\\ EXT/archive'"
     ]
    }
   ],
   "source": [
    "# Function\n",
    "def extract_json_from_directory(directory, search_word , col ):\n",
    "    \"\"\"Function to unzip and extract all json all files in a given directory\"\"\"\n",
    "    extracted_data = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.json.gz'):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            with gzip.open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    try:\n",
    "                        json_data = json.loads(line)\n",
    "                        extracted_data.append(json_data)\n",
    "\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error parsing JSON line in {file_name}: {e}\")\n",
    "        df = pd.DataFrame(extracted_data)\n",
    "        filtered = filter_tweets(df,search_word,col)\n",
    "    ftdf.append(filtered)\n",
    "        \n",
    "\n",
    "    \n",
    "        #for data in extracted_data:\n",
    "            #json.dump(data, output)\n",
    "    return ftdf\n",
    "directory = \"/Volumes/TOSHIBA\\ EXT/archive\"\n",
    "search_word = 'trans'\n",
    "df = extract_json_from_directory(directory,search_word,'text')\n",
    "\n",
    "# output_file = \"/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa1ace0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e519c316",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gtdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m cbdf \u001b[38;5;241m=\u001b[39m extract_json_from_directory(directory,search_words)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# gtdf = cbdf[['created_at','text']]\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[43mgtdf\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     38\u001b[0m gtdf\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSeaerched_tweets_result.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gtdf' is not defined"
     ]
    }
   ],
   "source": [
    "# Function copy to add jsonm search experiment ******************************************works\n",
    "def extract_json_from_directory(directory, search_words):\n",
    "    \"\"\"Function to unzip and extract all json all files in a given directory\"\"\"\n",
    "    extracted_data = []\n",
    "    my_df = pd.DataFrame()\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.json.gz'):\n",
    "            print(f\"Extracting file {file_name} \")\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            with gzip.open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    try:\n",
    "                        json_data = json.loads(line)\n",
    "                        extracted_data.append(json_data)\n",
    "\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error parsing JSON line in {file_name}: {e}\")\n",
    "                        \n",
    "                df = pd.DataFrame(extracted_data)\n",
    "                filtered_df = df[df[\"text\"].str.contains(\"gay|trans\")]\n",
    "                # filtered_df = df[df['text'].isin(search_words)]\n",
    "                \n",
    "                filtered_df = filtered_df[['created_at','text']]\n",
    "                my_df = pd.concat([filtered_df,my_df], ignore_index = True)\n",
    "                # my_df = my_df.append(filtered_df, ignore_index=True)\n",
    "             \n",
    "            \n",
    "            \n",
    "\n",
    "    return my_df\n",
    "\n",
    "\n",
    "search_words = ['trans','gay']\n",
    "directory = \"/Volumes/TOSHIBA_EXT/archive\"\n",
    "cbdf = extract_json_from_directory(directory,search_words)\n",
    "# gtdf = cbdf[['created_at','text']]\n",
    "gtdf.shape\n",
    "gtdf.to_csv('Seaerched_tweets_result.csv')\n",
    "\n",
    "# output_file = \"/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7cad82d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|date_extracted|tweet|\n",
      "+--------------+-----+\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up empty pyspark dataframe to store the imported tweets\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType,TimestampType\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate() #Start a spark session\n",
    "# Defining the schema of the pyspark dataframe\n",
    "schema = StructType([\n",
    "    StructField(\"date_extracted\", TimestampType(), True),\n",
    "    StructField(\"tweet\", StringType(), True)\n",
    "])\n",
    "stwit_df = spark.createDataFrame([], schema) # make empty dataframe\n",
    "# show the dataframe is as set out\n",
    "stwit_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c09870bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting file 0401_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0402_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0403_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0404_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0405_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0406_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0407_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0408_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0409_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0410_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0411_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0412_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0413_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0414_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0415_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0416_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0417_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0418_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0419_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0420_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0421_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0422_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0423_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0424_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0425_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0426_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0427_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0428_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0429_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0430_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0501_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0502_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0503_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0504_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0505_to_0507_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0508_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0509_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0510_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0511_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0512_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0513_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0514_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0515_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0516_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0517_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0518_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0519_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0520_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0521_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0522_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0523_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0524_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0525_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0526_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0527_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0528_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0529_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0530_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0531_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0601_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0602_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0603_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0604_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0605_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0606_to_08_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0609_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0610_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0611_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0612_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0613_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0614_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0615_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0616_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0617_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0618_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0619_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0620_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0621_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0622_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0623_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0624_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0625_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0626_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0627_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0628_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0629_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0630_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0701_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0702_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0703_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0704_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0705_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0706_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0707_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0708_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0709_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0710_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0711_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0712_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0713_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0714_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0715_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0716_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0717_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0718_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0719_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0720_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0721_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0722_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0723_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0724_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0725_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0726_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0727_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0728_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0729_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0730_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0731_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0801_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0802_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0803_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0804_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0805_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0806_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0807_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0808_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0809_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0810_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0811_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0812_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0813_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0814_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0815_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0816_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0817_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0818_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped20220227-131611.csv.gzip \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting file UkraineCombinedTweetsDeduped_FEB27.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_FEB28_part1.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_FEB28_part2.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR01.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR02.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR03.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR04.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR05.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR06.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR07.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR08.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR09.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR10.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR11.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR12.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR13.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR14.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR15.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR16.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR17.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR18.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR19.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR20.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR21.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR22.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR23.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR24.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR25.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR26.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR27_to_28.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR29.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR30.csv.gzip \n",
      "Extracting file UkraineCombinedTweetsDeduped_MAR31.csv.gzip \n",
      "Extracting file 0819_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0820_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0821_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0822_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0823_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0824_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0825_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0826_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0827_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0828_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0829_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0830_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0831_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0901_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0902_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0903_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0904_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0905_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0906_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0907_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0908_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0909_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0910_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0911_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0912_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0913_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0914_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0915_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0916_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0917_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0918_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0919_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0920_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0921_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0922_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0923_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0924_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0925_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0926_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0927_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0928_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0929_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 0930_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1001_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1002_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1003_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1004_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1005_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1006_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1007_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1008_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1009_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1010_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1011_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1012_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1013_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1014_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1015_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1016_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1017_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1018_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1019_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1020_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1021_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1022_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1023_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1024_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1025_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1026_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1027_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1028_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1029_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1030_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1031_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1101_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1102_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1103_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1104_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1105_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1106_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1107_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1108_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1109_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1110_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1111_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1112_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1113_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1114_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1115_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1116_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1117_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1118_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1119_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1120_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1121_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1122_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1123_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1124_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1125_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1126_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1127_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1128_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1129_UkraineCombinedTweetsDeduped.csv.gzip \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting file 1130_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1201_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1202_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1203_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1204_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1205_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1206_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1207_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1208_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1209_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1210_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1211_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1212_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1213_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1214_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1215_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1216_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1217_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1218_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1219_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1220_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1221_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1222_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1223_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1224_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1225_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1226_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1227_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1228_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1229_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1230_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 1231_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230101_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230102_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230103_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230104_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230105_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230106_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230107_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230108_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230109_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230110_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230111_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230112_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230113_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230114_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230115_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230116_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230117_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230118_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230119_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230120_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230121_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230122_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230123_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230124_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230125_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230126_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230127_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230128_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230129_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230130_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230131_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230201_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230202_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230203_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230204_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230205_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230206_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230207_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230208_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230209_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230210_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230211_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230212_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230213_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230214_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230215_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230216_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230217_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230218_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230219_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230220_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230221_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230222_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230223_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230224_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230225_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230226_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230227_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230228_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230301_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230302_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230303_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230304_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230305_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230306_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230307_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230308_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230309_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230310_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230311_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230312_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230313_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230314_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230315_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230316_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230317_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230318_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230319_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230320_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230321_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230322_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230323_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230324_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230325_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230326_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230327_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230328_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230329_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230330_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230331_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230401_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230402_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230403_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230404_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230405_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230406_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230407_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230408_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230409_UkraineCombinedTweetsDeduped.csv.gzip \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting file 20230410_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230411_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230412_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230413_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230414_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230415_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230416_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230417_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230418_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230419_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230420_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230421_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230422_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230423_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230424_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230425_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230426_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230427_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230428_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230429_to_20230430_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230501_to_20230502_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230503_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230504_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230505_to_20230508_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230509_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230510_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230511_to_20230512_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230513_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230514_UkraineCombinedTweetsDeduped.csv.gzip \n",
      "Extracting file 20230517_UkraineCombinedTweetsDeduped.csv.gzip \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-23 19:12:32,217 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "2023-05-23 19:12:33,257 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|     date_extracted|               tweet|\n",
      "+-------------------+--------------------+\n",
      "|2022-04-01 00:00:00|â—ï¸ðŸ«Every school ...|\n",
      "|2022-04-01 00:00:00|One year ago #Ukr...|\n",
      "|2022-04-01 00:00:00|JUST IN: #Anonymo...|\n",
      "|2022-04-01 00:00:00|Chernihiv oblast....|\n",
      "|2022-04-01 00:00:00|âš¡The Ukrainian Ai...|\n",
      "|2022-04-01 00:00:00|Help to save Ukra...|\n",
      "|2022-04-01 00:00:00|Help to save Ukra...|\n",
      "|2022-04-01 00:00:00|âœ¨ Angelina Jolie ...|\n",
      "|2022-04-01 00:00:00|This is not the t...|\n",
      "|2022-04-01 00:00:00|#notallrussians??...|\n",
      "|2022-04-01 00:00:00|âš¡The Ukrainian Ai...|\n",
      "|2022-04-01 00:00:00|ðŸ‡¬ðŸ‡§ðŸ‡ºðŸ‡¦How is th...|\n",
      "|2022-04-01 00:00:00|Grandmother with ...|\n",
      "|2022-04-01 00:00:00|An activist with ...|\n",
      "|2022-04-01 00:00:00|âš¡The Ukrainian Ai...|\n",
      "|2022-04-01 00:00:00|The first leader ...|\n",
      "|2022-04-01 00:00:00|The greatest army...|\n",
      "|2022-04-01 00:00:00|Ukrainian army so...|\n",
      "|2022-04-01 00:00:00|1/ Before tomorro...|\n",
      "|2022-04-01 00:00:00|#Latvia has banne...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 11:==========================================================(4 + 0) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Function copy to add gziped and search experiment *************************\n",
    "# ****** test ************* test *****************\n",
    "def extract_csv_from_directory(directory, stwit_df):\n",
    "    \"\"\"Function to unzip and extract all json all files in a given directory\"\"\"\n",
    "\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.csv.gzip'):\n",
    "            print(f\"Extracting file {file_name} \")\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            with gzip.open(file_path, 'r') as file:\n",
    "                df = pd.read_csv(file) #read the file as a pandas dataframe\n",
    "                filter1 = (df['language'] == 'en') & (df['location'] == 'Ukraine') # a filter for english language and location Ukraine\n",
    "                df = df[filter1] # Selects dataframe based on the filter\n",
    "                df = df[['extractedts','text']] # outputs only the columns im looking for date and text\n",
    "                # convert date to datetime object, any error times will be converted to NaT\n",
    "                df['extractedts'] = pd.to_datetime(df['extractedts'], errors='ignore').dt.date\n",
    "                df['text'] = df['text'].astype(str) # convert string to string object\n",
    "                sdf = spark.createDataFrame(df)\n",
    "                stwit_df = stwit_df.union(sdf)\n",
    "\n",
    "                        \n",
    "                #df = pd.DataFrame(extracted_data)\n",
    "                #filtered_df = df[df[\"text\"].str.contains(\"gay|trans\")]\n",
    "                # filtered_df = df[df['text'].isin(search_words)]\n",
    "                \n",
    "                #filtered_df = filtered_df[['created_at','text']]\n",
    "                #my_df = pd.concat([filtered_df,my_df], ignore_index = True)\n",
    "                # my_df = my_df.append(filtered_df, ignore_index=True)\n",
    "             \n",
    "            \n",
    "            \n",
    "\n",
    "    return stwit_df\n",
    "\n",
    "\n",
    "\n",
    "directory = \"/media/hduser/TOSHIBA_EXT/archive_copy\"\n",
    "cbdf = extract_csv_from_directory(directory,stwit_df)\n",
    "cbdf.show()\n",
    "\n",
    "\n",
    "# output_file = \"/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c5a62101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-23 22:08:40,220 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1554.7 KiB\n",
      "[Stage 12:=====================================================>(866 + 2) / 870]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of rows:372497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f'Count of rows:{cbdf.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c14c982c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-23 22:10:16,722 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "2023-05-23 22:11:19,522 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1029.3 KiB\n",
      "[Stage 15:==================================================>   (188 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of distinct rows: 299821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f'Count of distinct rows: {cbdf.distinct().count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5d9fa696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 72676 duplicate row\n",
    "cbdf = cbdf.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f446265e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-23 22:34:22,784 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "2023-05-23 22:35:22,059 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1190.3 KiB\n",
      "2023-05-23 22:35:27,535 WARN hdfs.DataStreamer: Caught exception (42 + 2) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save the file to Hadoop HDFS\n",
    "cbdf.write.parquet('hdfs://localhost:9000/user1/Ukraine_twitter.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22583c7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas==1.5.3\n",
      "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m842.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas==1.5.3) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/hduser/.local/lib/python3.10/site-packages (from pandas==1.5.3) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/hduser/.local/lib/python3.10/site-packages (from pandas==1.5.3) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.0.0\n",
      "    Uninstalling pandas-2.0.0:\n",
      "      Successfully uninstalled pandas-2.0.0\n",
      "Successfully installed pandas-1.5.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas==1.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82e00e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.3\n"
     ]
    }
   ],
   "source": [
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6c0063d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extractedts</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>2022-03-27 00:13:15.294765</td>\n",
       "      <td>ðŸ‡ºðŸ‡¦@ZelenskyyUa : \"I wish I could lend just a f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>2022-03-27 00:13:13.050305</td>\n",
       "      <td>check more #nft\\n@opensea\\n https://t.co/NHlo1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>2022-03-27 00:13:11.064423</td>\n",
       "      <td>@Reuters I would note that it is perfectly acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>2022-03-27 00:12:10.542924</td>\n",
       "      <td>These are not scenes from another fantasy movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>2022-03-27 00:31:27.710389</td>\n",
       "      <td>Our Anonymous brother @OpsAn0n  got suspended ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     extractedts  \\\n",
       "234   2022-03-27 00:13:15.294765   \n",
       "470   2022-03-27 00:13:13.050305   \n",
       "569   2022-03-27 00:13:11.064423   \n",
       "880   2022-03-27 00:12:10.542924   \n",
       "2341  2022-03-27 00:31:27.710389   \n",
       "\n",
       "                                                   text  \n",
       "234   ðŸ‡ºðŸ‡¦@ZelenskyyUa : \"I wish I could lend just a f...  \n",
       "470   check more #nft\\n@opensea\\n https://t.co/NHlo1...  \n",
       "569   @Reuters I would note that it is perfectly acc...  \n",
       "880   These are not scenes from another fantasy movi...  \n",
       "2341  Our Anonymous brother @OpsAn0n  got suspended ...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file_path = '/home/hduser/Desktop/S2_CA2/0819_UkraineCombinedTweetsDeduped.csv.gzip' \n",
    "file_path = '/media/hduser/TOSHIBA_EXT/archive_copy/UkraineCombinedTweetsDeduped_MAR27_to_28.csv.gzip'\n",
    "with gzip.open(file_path, 'rt') as file: # unzips the file\n",
    "    df = pd.read_csv(file) #read the file as a pandas dataframe\n",
    "    filter1 = (df['language'] == 'en') & (df['location'] == 'Ukraine') # a filtersfor english language and location Ukraine\n",
    "    df = df[filter1] # Selects dataframe based on the filter\n",
    "    df = df[['extractedts','text']] # outputs only the columns im looking for\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "47c47786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with missing values : 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of rows with missing values : {df.isnull().any(axis=1).mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5888c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['extractedts'] = pd.to_datetime(df['extractedts'], errors='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "477ad9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =df.set_index('extractedts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e4b92f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extractedts</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-03-27 00:13:15.294765</th>\n",
       "      <td>ðŸ‡ºðŸ‡¦@ZelenskyyUa : \"I wish I could lend just a f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-27 00:13:13.050305</th>\n",
       "      <td>check more #nft\\n@opensea\\n https://t.co/NHlo1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-27 00:13:11.064423</th>\n",
       "      <td>@Reuters I would note that it is perfectly acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-27 00:12:10.542924</th>\n",
       "      <td>These are not scenes from another fantasy movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-27 00:31:27.710389</th>\n",
       "      <td>Our Anonymous brother @OpsAn0n  got suspended ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                         text\n",
       "extractedts                                                                  \n",
       "2022-03-27 00:13:15.294765  ðŸ‡ºðŸ‡¦@ZelenskyyUa : \"I wish I could lend just a f...\n",
       "2022-03-27 00:13:13.050305  check more #nft\\n@opensea\\n https://t.co/NHlo1...\n",
       "2022-03-27 00:13:11.064423  @Reuters I would note that it is perfectly acc...\n",
       "2022-03-27 00:12:10.542924  These are not scenes from another fantasy movi...\n",
       "2022-03-27 00:31:27.710389  Our Anonymous brother @OpsAn0n  got suspended ..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49358cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('failing_date.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e9580dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3582 entries, 234 to 767845\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   extractedts  3582 non-null   datetime64[ns]\n",
      " 1   text         3582 non-null   object        \n",
      "dtypes: datetime64[ns](1), object(1)\n",
      "memory usage: 84.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cecf4161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>userid</th>\n",
       "      <th>username</th>\n",
       "      <th>acctdesc</th>\n",
       "      <th>location</th>\n",
       "      <th>following</th>\n",
       "      <th>followers</th>\n",
       "      <th>totaltweets</th>\n",
       "      <th>usercreatedts</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>...</th>\n",
       "      <th>original_tweet_userid</th>\n",
       "      <th>original_tweet_username</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>is_quote_status</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>quoted_status_userid</th>\n",
       "      <th>quoted_status_username</th>\n",
       "      <th>extractedts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>173212647</td>\n",
       "      <td>JoeMokolobetsi</td>\n",
       "      <td>Yeshua Hamashiach is THE answer | Romans 10:9-...</td>\n",
       "      <td>Afrika Borwa</td>\n",
       "      <td>219</td>\n",
       "      <td>197</td>\n",
       "      <td>4789</td>\n",
       "      <td>2010-07-31 19:09:22.000000</td>\n",
       "      <td>1560416252937617411</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-08-19 08:07:26.836769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>335041409</td>\n",
       "      <td>XclusivasPuebla</td>\n",
       "      <td>Somos el periÃ³dico  #ExclusivasPuebla| Investi...</td>\n",
       "      <td>Puebla, MÃ©xico</td>\n",
       "      <td>1419</td>\n",
       "      <td>6402</td>\n",
       "      <td>70267</td>\n",
       "      <td>2011-07-14 02:02:24.000000</td>\n",
       "      <td>1560416256179707904</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-08-19 07:51:50.523048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1512400441103032323</td>\n",
       "      <td>ShelterAnimalUA</td>\n",
       "      <td>Shelter for abandoned dogs and cats. 1400 dogs...</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>782</td>\n",
       "      <td>109</td>\n",
       "      <td>1198</td>\n",
       "      <td>2022-04-08 12:02:47.000000</td>\n",
       "      <td>1560416257752666113</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-08-19 05:12:06.194216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1356632630662430722</td>\n",
       "      <td>DogandCatHelpe1</td>\n",
       "      <td>Shelter for abandoned dogs and cats. 1400 dogs...</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>690</td>\n",
       "      <td>2021-02-02 15:57:12.000000</td>\n",
       "      <td>1560416257790382081</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-08-19 11:22:26.824532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20297125</td>\n",
       "      <td>ElMananaOnline</td>\n",
       "      <td>Las mejores noticias de los dos Laredos y el m...</td>\n",
       "      <td>Nuevo Laredo</td>\n",
       "      <td>2269</td>\n",
       "      <td>17978</td>\n",
       "      <td>56188</td>\n",
       "      <td>2009-02-07 06:32:49.000000</td>\n",
       "      <td>1560416257937051648</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-08-19 11:52:29.448634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0               userid         username   \n",
       "0           0            173212647   JoeMokolobetsi  \\\n",
       "1           1            335041409  XclusivasPuebla   \n",
       "2           2  1512400441103032323  ShelterAnimalUA   \n",
       "3           3  1356632630662430722  DogandCatHelpe1   \n",
       "4           4             20297125   ElMananaOnline   \n",
       "\n",
       "                                            acctdesc        location   \n",
       "0  Yeshua Hamashiach is THE answer | Romans 10:9-...    Afrika Borwa  \\\n",
       "1  Somos el periÃ³dico  #ExclusivasPuebla| Investi...  Puebla, MÃ©xico   \n",
       "2  Shelter for abandoned dogs and cats. 1400 dogs...         Ukraine   \n",
       "3  Shelter for abandoned dogs and cats. 1400 dogs...         Ukraine   \n",
       "4  Las mejores noticias de los dos Laredos y el m...    Nuevo Laredo   \n",
       "\n",
       "   following  followers  totaltweets               usercreatedts   \n",
       "0        219        197         4789  2010-07-31 19:09:22.000000  \\\n",
       "1       1419       6402        70267  2011-07-14 02:02:24.000000   \n",
       "2        782        109         1198  2022-04-08 12:02:47.000000   \n",
       "3          5         39          690  2021-02-02 15:57:12.000000   \n",
       "4       2269      17978        56188  2009-02-07 06:32:49.000000   \n",
       "\n",
       "               tweetid  ... original_tweet_userid  original_tweet_username   \n",
       "0  1560416252937617411  ...                     0                      NaN  \\\n",
       "1  1560416256179707904  ...                     0                      NaN   \n",
       "2  1560416257752666113  ...                     0                      NaN   \n",
       "3  1560416257790382081  ...                     0                      NaN   \n",
       "4  1560416257937051648  ...                     0                      NaN   \n",
       "\n",
       "  in_reply_to_status_id in_reply_to_user_id in_reply_to_screen_name   \n",
       "0                     0                   0                     NaN  \\\n",
       "1                     0                   0                     NaN   \n",
       "2                     0                   0                     NaN   \n",
       "3                     0                   0                     NaN   \n",
       "4                     0                   0                     NaN   \n",
       "\n",
       "  is_quote_status  quoted_status_id  quoted_status_userid   \n",
       "0           False                 0                     0  \\\n",
       "1           False                 0                     0   \n",
       "2           False                 0                     0   \n",
       "3           False                 0                     0   \n",
       "4           False                 0                     0   \n",
       "\n",
       "   quoted_status_username                 extractedts  \n",
       "0                     NaN  2022-08-19 08:07:26.836769  \n",
       "1                     NaN  2022-08-19 07:51:50.523048  \n",
       "2                     NaN  2022-08-19 05:12:06.194216  \n",
       "3                     NaN  2022-08-19 11:22:26.824532  \n",
       "4                     NaN  2022-08-19 11:52:29.448634  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = '/home/hduser/Desktop/S2_CA2/extracted_data.csv'\n",
    "df = pd.read_csv(file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5189573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47994 entries, 0 to 47993\n",
      "Data columns (total 29 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Unnamed: 0               47994 non-null  int64  \n",
      " 1   userid                   47994 non-null  int64  \n",
      " 2   username                 47994 non-null  object \n",
      " 3   acctdesc                 43037 non-null  object \n",
      " 4   location                 31703 non-null  object \n",
      " 5   following                47994 non-null  int64  \n",
      " 6   followers                47994 non-null  int64  \n",
      " 7   totaltweets              47994 non-null  int64  \n",
      " 8   usercreatedts            47994 non-null  object \n",
      " 9   tweetid                  47994 non-null  int64  \n",
      " 10  tweetcreatedts           47994 non-null  object \n",
      " 11  retweetcount             47994 non-null  int64  \n",
      " 12  text                     47994 non-null  object \n",
      " 13  hashtags                 47994 non-null  object \n",
      " 14  language                 47994 non-null  object \n",
      " 15  coordinates              189 non-null    object \n",
      " 16  favorite_count           47994 non-null  int64  \n",
      " 17  is_retweet               47994 non-null  bool   \n",
      " 18  original_tweet_id        47994 non-null  int64  \n",
      " 19  original_tweet_userid    47994 non-null  int64  \n",
      " 20  original_tweet_username  0 non-null      float64\n",
      " 21  in_reply_to_status_id    47994 non-null  int64  \n",
      " 22  in_reply_to_user_id      47994 non-null  int64  \n",
      " 23  in_reply_to_screen_name  13240 non-null  object \n",
      " 24  is_quote_status          47994 non-null  bool   \n",
      " 25  quoted_status_id         47994 non-null  int64  \n",
      " 26  quoted_status_userid     47994 non-null  int64  \n",
      " 27  quoted_status_username   7089 non-null   object \n",
      " 28  extractedts              47994 non-null  object \n",
      "dtypes: bool(2), float64(1), int64(14), object(12)\n",
      "memory usage: 10.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06474036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location\n",
      "Ukraine                           1233\n",
      "Ð£ÐºÑ€Ð°Ñ—Ð½Ð°                            946\n",
      "Ð£ÐºÑ€Ð°Ð¸Ð½Ð°                            517\n",
      "United States                      378\n",
      "Bay Area, CA                       312\n",
      "                                  ... \n",
      "Novi Sad, Serbia                     1\n",
      "Atacama, Chile                       1\n",
      "Over at the Frankenstein Place       1\n",
      "Birmingham, West Midlands            1\n",
      "Orange County , CA.                  1\n",
      "Name: count, Length: 7134, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "item_count = df['location'].value_counts()\n",
    "print(item_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4de237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test loading json and searching in one step\n",
    "import json\n",
    "\n",
    "def search_json(json_file, search_terms):\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    results = []\n",
    "    for item in data:\n",
    "        text = item.get('text', '')  # Change 'text' to the appropriate key in your JSON structure\n",
    "        if all(term in text.lower() for term in search_terms):\n",
    "            results.append(text)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "json_file = 'data.json'  # Replace with your JSON file path\n",
    "search_terms = ['term1', 'term2', 'term3']  # Replace with your search terms\n",
    "\n",
    "matching_texts = search_json(json_file, search_terms)\n",
    "if matching_texts:\n",
    "    print(\"Matching texts:\")\n",
    "    for text in matching_texts:\n",
    "        print(text)\n",
    "else:\n",
    "    print(\"No matching texts found.\")\n",
    "```\n",
    "\n",
    "Make sure to replace `'text'` with the appropriate key that contains the text string within your JSON structure. Also, update the `json_file` variable with the path to your actual JSON file, and `search_terms` with the set of terms you want to search for.\n",
    "\n",
    "The script will print all the text strings from the JSON file that contain all the search terms. If no matching texts are found, it will display \"No matching texts found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "755bb40a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Trailing data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_results\n\u001b[1;32m     11\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data/20220901\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m cbdf \u001b[38;5;241m=\u001b[39m \u001b[43mload_json_files_and_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m cbdf\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36mload_json_files_and_search\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      5\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, file_name)\n\u001b[0;32m----> 6\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     search_results \u001b[38;5;241m=\u001b[39m filter_tweets(df,search_word,col)\n\u001b[1;32m      8\u001b[0m     combined_results \u001b[38;5;241m=\u001b[39m combined_results\u001b[38;5;241m.\u001b[39mappend(search_results, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/json/_json.py:757\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m json_reader:\n\u001b[0;32m--> 757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/json/_json.py:915\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_lines(data_lines))\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 915\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/json/_json.py:937\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    935\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 937\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/json/_json.py:1064\u001b[0m, in \u001b[0;36mParser.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_numpy()\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_no_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/json/_json.py:1321\u001b[0m, in \u001b[0;36mFrameParser._parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1317\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[0;32m-> 1321\u001b[0m         \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m     )\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1324\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1325\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[1;32m   1326\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1327\u001b[0m     }\n",
      "\u001b[0;31mValueError\u001b[0m: Trailing data"
     ]
    }
   ],
   "source": [
    "def load_json_files_and_search(directory):\n",
    "    combined_results = pd.DataFrame()\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.json.gz'):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            df = pd.read_json(file_path)\n",
    "            search_results = filter_tweets(df,search_word,col)\n",
    "            combined_results = combined_results.append(search_results, ignore_index=True)\n",
    "    return combined_results\n",
    "\n",
    "directory = \"/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data/20220901\"\n",
    "cbdf = load_json_files_and_search(directory)\n",
    "cbdf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ed42292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function loads the json files one at a time and converts them into dataframe\n",
    "def convert_json_to_dataframe(json_file):\n",
    "    \"\"\"A simple function to convert json file to dataframe\"\"\"\n",
    "    data = []\n",
    "    with open(json_file, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                json_data = json.loads(line)\n",
    "                data.append(json_data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing JSON line: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n",
    "# twit_file = '20220901000000.json'\n",
    "# Keeping the json files outside my git repository , to reduce git size\n",
    "#twit_file = \"/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data/20220901/20220901000000.json\"\n",
    "#df = convert_json_to_dataframe(twit_file)\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac47b595",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "extract_json_from_directory() missing 2 required positional arguments: 'search_word' and 'col'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data/20220901\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Call function to extract tweet directory\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m combined_json \u001b[38;5;241m=\u001b[39m \u001b[43mextract_json_from_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: extract_json_from_directory() missing 2 required positional arguments: 'search_word' and 'col'"
     ]
    }
   ],
   "source": [
    "# path to my zipped tweets\n",
    "directory = \"/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_BD_ADA_CA2/Twitter_Archive_Data/20220901\"\n",
    "# Call function to extract tweet directory\n",
    "combined_json = extract_json_from_directory(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9916e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = convert_json_to_dataframe(combined_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa8bd60",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m search_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgas\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m fdf \u001b[38;5;241m=\u001b[39m filter_tweets(\u001b[43mdf\u001b[49m, col , search_word)\n\u001b[1;32m      9\u001b[0m fdf\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "def filter_tweets(df, col , search_word):\n",
    "    filtered_tweets = df[df[col].apply(lambda x: search_word.lower() in x.lower().split())]\n",
    "    return filtered_tweets\n",
    "\n",
    "\n",
    "search_word = 'gas'\n",
    "col = 'text'\n",
    "fdf = filter_tweets(df, col , search_word)\n",
    "fdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8392a47a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
