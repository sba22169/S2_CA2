{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "753a892c",
   "metadata": {},
   "source": [
    "## CA2_A is about pulling the twitter data and saving it as a file to be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55fca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # We can suppress the warnings\n",
    "# my twitter keys are stores in a file outside the git repository for security reasons\n",
    "config = dotenv_values(\"/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_DS_CA2/.env\")\n",
    "bearer_token = config[\"BEARER_TOKEN\"]\n",
    "# this is the location of the twitter api access , looking for the most recent tweets\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/recent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f703e7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set up the query we want to pass to api , is a json object\n",
    "    #   Im adding parameter -is = is not a retweet. i dont want the same retweets showing up in my data \n",
    "    # also pulling in the author id and name\n",
    "# Im using two query_params the first is without the next_token field for the first query \n",
    "start_time = \"2023-05-01T00:00:00Z\"\n",
    "end_time = \"2023-05-04T00:00:00Z\"\n",
    "query_params1 = {f'query':'Russian war -is:retweet',\n",
    "    'start_time':{start_time},\n",
    "    'end_time':{end_time},\n",
    "    'max_results': '100',\n",
    "    'tweet.fields':'author_id',\n",
    "    'user.fields':'name',\n",
    "\n",
    "}\n",
    "# This second query is for the loop , as it requires a next_token.\n",
    "query_params2 = {f'query':'Russia war -is:retweet',\n",
    "    'start_time':{start_time},\n",
    "    'end_time':{end_time},\n",
    "    'max_results': '100',\n",
    "    'tweet.fields':'author_id',\n",
    "    'user.fields':'name',\n",
    "    'next_token' : 'abcd',\n",
    "}\n",
    "\n",
    "# my authorisation keys are used here for the twitter API\n",
    "def bearer_oauth(r):\n",
    "    \"\"\" Function for using bearer token\"\"\"\n",
    "    r.headers['Authorization'] = f\"Bearer {bearer_token}\"\n",
    "    r.headers['User-Agent'] = \"v2RecentSerchPython\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url,params):\n",
    "    \"\"\" Function to connect to twitter API\"\"\"\n",
    "    response = requests.get(url,auth=bearer_oauth, params=params)\n",
    "    #responce.status_code 200 is good anything else is an error\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "def append_dict(adict,json_response):\n",
    "    for i in range(len(json_response['data'])):\n",
    "        try:\n",
    "            adict[json_response['data'][i]['author_id']] = json_response['data'][i]['text']\n",
    "\n",
    "        except IndexError:\n",
    "            print(\"We had an index error\")\n",
    "    return(adict)\n",
    "\n",
    "    \n",
    "def twit_call():\n",
    "    twit_dict={}\n",
    "    loop_count = 0\n",
    "    json_response = connect_to_endpoint(search_url, query_params1)\n",
    "    print(json_response)\n",
    "\n",
    "    append_dict(twit_dict,json_response)\n",
    "    # while json_response['meta']['next_token']:\n",
    "    while True:\n",
    "        # pagenation is taking the next_token and feeding it through to next query\n",
    "        if 'next_token' in json_response['meta']:\n",
    "            print(\"Yes next token\")\n",
    "            next_t = json_response['meta']['next_token']\n",
    "            query_params2['next_token'] = next_t\n",
    "        else:\n",
    "            # this is the last page\n",
    "            break\n",
    "        json_response = connect_to_endpoint(search_url, query_params2)\n",
    "        append_dict(twit_dict,json_response)\n",
    "           \n",
    "    return(twit_dict)\n",
    "\n",
    "my_dict = twit_call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76837bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dictionary to dataframe\n",
    "df = pd.DataFrame.from_dict(my_dict, orient='index',columns=['tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2414b460",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d266cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d39fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe as cvs file.\n",
    "\n",
    "df.to_csv('Russia_War_TWEETS.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
